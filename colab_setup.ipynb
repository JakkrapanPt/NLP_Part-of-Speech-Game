{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# üöÄ Parts of Speech Game - LLM API Server\n",
        "\n",
        "This notebook sets up an API server for generating sentences using LLM models.\n",
        "\n",
        "## üìã Instructions:\n",
        "1. Run all cells in order\n",
        "2. Copy the ngrok URL from the output\n",
        "3. Use this URL in your local Streamlit app\n",
        "\n",
        "## ‚ö†Ô∏è Important Notes:\n",
        "- Keep this notebook running while using the local app\n",
        "- The ngrok URL changes each time you restart\n",
        "- Free ngrok has usage limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install flask flask-cors pyngrok transformers torch openai\n",
        "\n",
        "# Install PyThaiNLP for Thai language support\n",
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_ngrok"
      },
      "outputs": [],
      "source": [
        "# Set up ngrok authentication (optional but recommended)\n",
        "# Get your auth token from https://ngrok.com/\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Uncomment and add your ngrok auth token for better stability\n",
        "# ngrok.set_auth_token(\"2xdXSqaLA8wPwljc2izXFVfaqxq_3vL4WrgXXJ8zo6UM6CugP\")\n",
        "\n",
        "print(\"‚úÖ Ngrok setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_server_code"
      },
      "outputs": [],
      "source": [
        "# Create the API server code\n",
        "\n",
        "api_server_code = '''\n",
        "# Google Colab API Server for Parts of Speech Game\n",
        "# This file should be run in Google Colab to serve the LLM API\n",
        "\n",
        "import os\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "import openai\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for cross-origin requests\n",
        "\n",
        "# Global variables for models\n",
        "llm_pipeline = None\n",
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'model_name': 'microsoft/DialoGPT-medium',  # You can change this to other models\n",
        "    'max_length': 100,\n",
        "    'temperature': 0.7,\n",
        "    'use_openai': False,  # Set to True if you want to use OpenAI API\n",
        "    'openai_api_key': None  # Set your OpenAI API key here\n",
        "}\n",
        "\n",
        "def initialize_model():\n",
        "    \"\"\"Initialize the language model\"\"\"\n",
        "    global llm_pipeline, tokenizer, model\n",
        "    \n",
        "    print(\"Initializing language model...\")\n",
        "    \n",
        "    if CONFIG['use_openai'] and CONFIG['openai_api_key']:\n",
        "        # Use OpenAI API\n",
        "        openai.api_key = CONFIG['openai_api_key']\n",
        "        print(\"Using OpenAI API\")\n",
        "    else:\n",
        "        # Use Hugging Face transformers\n",
        "        try:\n",
        "            device = 0 if torch.cuda.is_available() else -1\n",
        "            print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
        "            \n",
        "            # Load model and tokenizer\n",
        "            tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "            model = AutoModelForCausalLM.from_pretrained(CONFIG['model_name'])\n",
        "            \n",
        "            # Create text generation pipeline\n",
        "            llm_pipeline = pipeline(\n",
        "                'text-generation',\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device,\n",
        "                max_length=CONFIG['max_length'],\n",
        "                temperature=CONFIG['temperature']\n",
        "            )\n",
        "            print(f\"Model {CONFIG['model_name']} loaded successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            # Fallback to a smaller model\n",
        "            try:\n",
        "                llm_pipeline = pipeline('text-generation', model='gpt2')\n",
        "                print(\"Fallback to GPT-2 model\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Error loading fallback model: {e2}\")\n",
        "                llm_pipeline = None\n",
        "\n",
        "# Sample sentences as fallback\n",
        "FALLBACK_SENTENCES = {\n",
        "    'en': {\n",
        "        'easy': [\n",
        "            \"The cat sleeps peacefully.\",\n",
        "            \"She runs quickly today.\",\n",
        "            \"Birds fly high above.\",\n",
        "            \"We eat delicious food.\"\n",
        "        ],\n",
        "        'medium': [\n",
        "            \"The beautiful flowers bloom in spring garden.\",\n",
        "            \"Students study hard for their important exams.\",\n",
        "            \"My grandmother tells interesting stories every evening.\"\n",
        "        ],\n",
        "        'hard': [\n",
        "            \"The magnificent orchestra performed brilliantly at the prestigious concert hall last night.\",\n",
        "            \"Scientists carefully analyze complex data to understand mysterious phenomena in deep space.\"\n",
        "        ]\n",
        "    },\n",
        "    'th': {\n",
        "        'easy': [\n",
        "            \"‡πÅ‡∏°‡∏ß‡∏ô‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏ö‡∏™‡∏ö‡∏≤‡∏¢\",\n",
        "            \"‡πÄ‡∏Ç‡∏≤‡∏ß‡∏¥‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å\",\n",
        "            \"‡∏ô‡∏Å‡∏ö‡∏¥‡∏ô‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô\",\n",
        "            \"‡πÄ‡∏£‡∏≤‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏≠‡∏£‡πà‡∏≠‡∏¢\"\n",
        "        ],\n",
        "        'medium': [\n",
        "            \"‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ç‡∏¢‡∏±‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≠‡∏ö\",\n",
        "            \"‡∏î‡∏≠‡∏Å‡πÑ‡∏°‡πâ‡∏™‡∏ß‡∏¢‡∏ö‡∏≤‡∏ô‡πÉ‡∏ô‡∏™‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ö‡πâ‡∏≤‡∏ô\",\n",
        "            \"‡∏Ñ‡∏∏‡∏ì‡∏¢‡∏≤‡∏¢‡πÄ‡∏•‡πà‡∏≤‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏∑‡∏ô\"\n",
        "        ],\n",
        "        'hard': [\n",
        "            \"‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏•‡∏∂‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô‡∏≠‡∏ß‡∏Å‡∏≤‡∏®\",\n",
        "            \"‡∏ß‡∏á‡∏î‡∏∏‡∏£‡∏¥‡∏¢‡∏≤‡∏á‡∏Ñ‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏î‡∏±‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡πÉ‡∏ô‡∏´‡∏≠‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return jsonify({\n",
        "        'status': 'healthy',\n",
        "        'model_loaded': llm_pipeline is not None or CONFIG['use_openai'],\n",
        "        'timestamp': time.time()\n",
        "    })\n",
        "\n",
        "@app.route('/generate_sentence', methods=['POST'])\n",
        "def generate_sentence_api():\n",
        "    \"\"\"API endpoint to generate sentences\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        \n",
        "        # Extract parameters\n",
        "        language = data.get('language', 'en')\n",
        "        difficulty = data.get('difficulty', 'easy')\n",
        "        \n",
        "        # Fallback to sample sentences\n",
        "        import random\n",
        "        sentences = FALLBACK_SENTENCES.get(language, {}).get(difficulty, [])\n",
        "        if sentences:\n",
        "            sentence = random.choice(sentences)\n",
        "        else:\n",
        "            sentence = \"The quick brown fox jumps.\" if language == 'en' else \"‡πÅ‡∏°‡∏ß‡∏î‡∏≥‡∏ß‡∏¥‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß\"\n",
        "        \n",
        "        return jsonify({\n",
        "            'success': True,\n",
        "            'sentence': sentence,\n",
        "            'language': language,\n",
        "            'difficulty': difficulty,\n",
        "            'method': 'fallback'\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            'success': False,\n",
        "            'error': str(e)\n",
        "        }), 500\n",
        "\n",
        "def run_server():\n",
        "    \"\"\"Run the Flask server\"\"\"\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "\n",
        "def setup_ngrok():\n",
        "    \"\"\"Setup ngrok tunnel for external access\"\"\"\n",
        "    try:\n",
        "        # Create tunnel\n",
        "        public_url = ngrok.connect(5000)\n",
        "        print(f\"\\\\nüåê Public URL: {public_url}\")\n",
        "        print(f\"üì± Use this URL in your Streamlit app: {public_url}\")\n",
        "        print(f\"üîó API endpoint: {public_url}/generate_sentence\")\n",
        "        \n",
        "        return public_url\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"üöÄ Starting Parts of Speech Game API Server\")\n",
        "    \n",
        "    print(\"üåê Setting up ngrok tunnel...\")\n",
        "    public_url = setup_ngrok()\n",
        "    \n",
        "    print(\"\\nüéØ API Endpoints:\")\n",
        "    print(\"  - POST /generate_sentence - Generate sentences\")\n",
        "    print(\"  - GET /health - Health check\")\n",
        "    \n",
        "    print(\"\\nüî• Starting Flask server...\")\n",
        "    \n",
        "    # Start the server\n",
        "    try:\n",
        "        run_server()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüëã Server stopped\")\n",
        "        ngrok.disconnect(public_url)\n",
        "        ngrok.kill()\n",
        "'''\n",
        "\n",
        "# Write the server code to a file\n",
        "with open('api_server.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(api_server_code)\n",
        "\n",
        "print(\"‚úÖ API server code created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_server"
      },
      "outputs": [],
      "source": [
        "# Run the API server\n",
        "\n",
        "print(\"üöÄ Starting the API server...\")\n",
        "print(\"‚ö†Ô∏è  Keep this cell running while using the local Streamlit app\")\n",
        "print(\"üìã Copy the ngrok URL from the output below\")\n",
        "print(\"\" + \"=\" * 50)\n",
        "\n",
        "# Execute the server\n",
        "exec(open('fixed_colab_api_server_new.py').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_instructions"
      },
      "source": [
        "## üìù Usage Instructions\n",
        "\n",
        "1. **Copy the ngrok URL** from the output above (looks like `https://xxxx-xx-xx-xx-xx.ngrok-free.app`)\n",
        "\n",
        "2. **In your local Streamlit app:**\n",
        "   - Check \"Use LLM API\"\n",
        "   - Paste the ngrok URL\n",
        "   - Click \"Test API Connection\"\n",
        "   - Start playing the game!\n",
        "\n",
        "3. **Keep this notebook running** while using the local app\n",
        "\n",
        "## üîß Customization\n",
        "\n",
        "You can modify the `CONFIG` dictionary in the server code to:\n",
        "- Use different models\n",
        "- Add OpenAI API support\n",
        "- Adjust generation parameters\n",
        "\n",
        "## ‚ö†Ô∏è Important Notes\n",
        "\n",
        "- **Free ngrok limits:** 1 tunnel, 40 connections/minute\n",
        "- **Colab timeout:** Sessions may timeout after inactivity\n",
        "- **URL changes:** New ngrok URL each time you restart\n",
        "- **HTTPS required:** Some browsers require HTTPS for API calls"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
